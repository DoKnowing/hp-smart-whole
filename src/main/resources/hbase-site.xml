<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Copyright 2010 The Apache Software Foundation
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://smartcluster/hbase</value>
        <description>The directory shared by region servers.</description>
    </property>
    <property>
        <name>hbase.assignment.already.intransition.waittime</name>
        <value>300000</value>
    </property>
    <property>
        <name>hbase.regionserver.executor.openregion.threads</name>
        <value>15</value>
    </property>
    <property>
        <name>hbase.regionserver.executor.closeregion.threads</name>
        <value>15</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>smart-master,smart-slave1,smart-slave2</value>
        <description>Comma separated list of servers in the ZooKeeper ensemble.</description>
    </property>
    <property>
        <name>hbase.local.dir</name>
        <value>/opt/software/hadoop/hbase/local/</value>
        <description>Directory on the local filesystem to be used as a local storage.
            th actual local-path is: hbase.local.dir/jars/
            in hbase-default.xml: ${hbase.tmp.dir}/local/(/tmp/local/)
        </description>
    </property>
    <property>
        <name>hbase.regionserver.handler.count</name>
        <value>200</value>
        <description>Count of RPC Listener instances spun up on RegionServers. Same property is used by the Master for
            count of master handlers. in hbase-default.xml: 30
        </description>
    </property>
    <property>
        <name>hbase.regionserver.global.memstore.upperLimit</name>
        <value>0.40</value>
        <description>Maximum size of all memstores in a region server before new updates are blocked and flushes are
            forced. Defaults to 40% of heap.
            Updates are blocked and flushes are forced until size of all memstores in a region server hits
            hbase.regionserver.global.memstore.lowerLimit.
        </description>
    </property>

    <property>
        <name>hbase.regionserver.global.memstore.lowerLimit</name>
        <value>0.35</value>
        <description>When memstores are being forced to flush to make room in memory, keep flushing until we hit this
            mark. Defaults to 38% of heap.
            This value equal to hbase.regionserver.global.memstore.upperLimit causes the minimum possible flushing to
            occur when updates are blocked due to memstore limiting.
        </description>
    </property>
    <property>
        <name>zookeeper.session.timeout</name>
        <value>240000</value>
        <description>ZooKeeper session timeout in milliseconds.
            It is used in two different ways.
            First, this value is used in the ZK client that HBase uses to connect to the ensemble.
            It is also used by HBase when it starts a ZK server and it is passed as the 'maxSessionTimeout'.
        </description>
    </property>

    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
        <description>Property from ZooKeeper's config zoo.cfg. The port at which the clients will connect.</description>
    </property>

    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/opt/software/hadoop/zookeeper/data</value>
        <description>Property from ZooKeeper's config zoo.cfg. The directory where the snapshot is stored.
            use a seperate disk from datanode, can make zk runs more quickly.
        </description>
    </property>
    <property>
        <name>hbase.client.scanner.timeout.period</name>
        <value>180000</value>
        <description>Client scanner lease period in milliseconds.
            0.94: hbase.regionserver.lease.period,
            0.96:
            hbase-default: 60s
        </description>
    </property>

    <property>
        <name>hbase.htable.threads.max</name>
        <value>20</value>
        <description>threads number for htable do work</description>
    </property>
    <property>
        <name>hbase.bulkload.retries.number</name>
        <value>5</value>
        <description>Maximum retries.
            This is maximum number of iterations to atomic bulk loads are attempted in the face of splitting operations.
            0 means never give up. Default: 0.
        </description>
    </property>

    <property>
        <name>hbase.regions.slop</name>
        <value>0.2</value>
        <description>Rebalance if any regionserver has average + (average * slop) regions. hbase-default: 0.2.
        </description>
    </property>
    <property>
        <name>hbase.hregion.memstore.flush.size</name>
        <value>134217728</value>
        <description>268435456(256M)Memstore will be flushed to disk if size of the memstore exceeds this number of
            bytes.
            Value is checked by a thread that runs every hbase.server.thread.wakefrequency.
            hbase-default: 134217728(128M)
        </description>
    </property>

    <property>
        <name>hbase.hregion.max.filesize</name>
        <value>42949672960</value>
        <description>Maximum HStoreFile size.
            If any one of a column families' HStoreFiles has grown to exceed this value, the hosting HRegion is split in
            two. Default: 10G.
            Actually: this conf is for a whole store, not for a store file.
        </description>
    </property>

    <property>
        <name>hbase.hstore.compactionThreshold</name>
        <value>7</value>
        <description>if more than this number of HStoreFiles in any one HStore(one HStoreFile is written per flush of
            memstore),
            then a compaction is run to rewrite all HStoreFiles files as one.
            Larger numbers put off compaction but when it runs, it takes longer to complete.
            new conf name : hbase.hstore.compaction.min
            hbase-default: 3
        </description>
    </property>
    <property>
        <name>hbase.hstore.compaction.min</name>
        <value>7</value>
        <description>same with conf hbase.hstore.compactionThreshold.</description>
    </property>

    <property>
        <name>hbase.hstore.compaction.max</name>
        <value>15</value>
        <description>Max number of HStoreFiles to compact per 'minor' compaction.
            hbase-default: 10
        </description>
    </property>

    <property>
        <name>hbase.hstore.blockingStoreFiles</name>
        <value>20</value>
        <description>if more than this number of StoreFiles in any one Store(one StoreFile is written per flush of
            MemStore) then updates are
            blocked for this HRegion until a compaction is completed, or until hbase.hstore.blockingWaitTime has been
            exceeded.
            hbase-default: 10
        </description>
    </property>
    <property>
        <name>hbase.hregion.majorcompaction</name>
        <value>432000000</value>
        <description>The time (in miliseconds) between 'major' compactions of all HStoreFiles in a region.
            Major compactions tend to happen exactly when you need them least so enable them such that they run at
            off-peak for your deploy;
            or, since this setting is on a periodicity that is unlikely to match your loading, run the compactions via
            an external invocation
            out of a cron job or some such.
            Set to 0 to disable automated major compactions.
            Default: Set to 7 days.
            majorCompactionPeriod
        </description>
    </property>

    <property>
        <name>hbase.regionserver.thread.compaction.throttle</name>
        <value>5368709120</value>
        <description>the throttle to determine put this compact to large/small pool, in bytes, 8687091200=8G
        </description>
    </property>

    <property>
        <name>hbase.regionserver.thread.compaction.small</name>
        <value>3</value>
        <description>the thread pool size of small compaction, not in hbase-default, default value: 1</description>
    </property>

    <property>
        <name>hbase.regionserver.thread.compaction.large</name>
        <value>3</value>
        <description>the thread pool size of large compaction, not in hbase-default, default value: 1</description>
    </property>
    <property>
        <name>hbase.hstore.compaction.kv.max</name>
        <value>200</value>
        <description>How many KeyValues to read and then write in a batch when flushing or compacting.
            Do less if big KeyValues and problems with OOME. Do more if wide, small rows.
            hbase-default: 10
        </description>
    </property>

    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>

    <property>
        <name>hbase.storescanner.parallel.seek.enable</name>
        <value>true</value>
        <description>Enables StoreFileScanner parallel-seeking in StoreScanner, a feature which can reduce response
            latency under special conditions.
        </description>
    </property>
    <property>
        <name>hbase.dynamic.jars.dir</name>
        <value>${hbase.rootdir}/lib</value>
        <description>The directory from which the custom filter/co-processor jars can be loaded dynamically by the
            region server without the need to restart.
            However, an already loaded filter/co-processor class would not be un-loaded. See HBASE-1936 for more
            details.
            we can put the jars need to update dynamically to this HDFS path, these jars can be loaded to every RS
            dynamically, copy to hbase.local.jars
            in hbase-default.xml: ${hbase.rootdir}/lib
        </description>
    </property>

    <property>
        <name>hbase.hstore.defaultengine.compactionpolicy.class</name>
        <value>org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy</value>
        <description>hbase0.94.2 default compaction policy:RatioBasedCompactionPolicy
            hbase0.98.10 default compaction policy:ExploringCompactionPolicy
            find rs dead in guiZhou, modify policy from ExploringCompactionPolicy to RatioBasedCompactionPolicy has
            effect
        </description>
    </property>
    <property>
        <name>hbase.hregion.memstore.chunkpool.maxsize</name>
        <value>0.5</value>
    </property>
    <property>
        <name>hbase.replication</name>
        <value>false</value>
    </property>
    <property>
        <name>hbase.snapshot.master.timeout.millis</name>
        <value>600000</value>
        <description>used for master to get timeout for snapshot, hbaseAdmin will get this value and wait, if timeout,
            client exception
        </description>
    </property>
    <property>
        <name>hbase.regionserver.region.split.policy</name>
        <value>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy</value>
        <description>after hbase0.94.2, the default region split policy is: IncreasingToUpperBoundRegionSplitPolicy,
            which may lead to region split too quickly.
        </description>
    </property>

    <property>
        <name>hbase.hstore.compaction.max.size</name>
        <value>10000000000</value>
        <description>in minor compaction, can skip large HFiles when selection, this conf is used to set this skip-size
            default: Long.MAX_VALUE, do not skip any files 10000000000: 10G home-jira: HBASE-62
        </description>
    </property>
    <property>
        <name>hbase.server.compactchecker.interval.multiplier</name>
        <value>43200</value>
        <description>compaction checker runs every 10s, this conf is the store's multiplier, 43200*10=43200s=5day
            has effect on rs, add to master/rs
            home-jira: HBASE-129
        </description>
    </property>

    <property>
        <name>hbase.offpeak.start.hour</name>
        <value>19</value>
        <description>minor compact will use hbase.hstore.compaction.ratio.offpeak to select HFile between start and end
            hour
            has effect on rs, add to master/rs
            home-jira: HBASE-133
        </description>
    </property>
    <property>
        <name>hbase.offpeak.end.hour</name>
        <value>7</value>
        <description>minor compact will use hbase.hstore.compaction.ratio.offpeak to select HFile between start and end
            hour
            has effect on rs, add to master/rs
            home-jira: HBASE-133
        </description>
    </property>
    <property>
        <name>hbase.hstore.compaction.ratio.offpeak</name>
        <value>1.2</value>
        <description>minor compact will use hbase.hstore.compaction.ratio.offpeak to select HFile between start and end
            hour
            has effect on rs, add to master/rs
            home-jira: HBASE-133
        </description>
    </property>
    <property>
        <name>hbase.hstore.compaction.ratio</name>
        <value>0.00001</value>
        <description>minor compact will use hbase.hstore.compaction.ratio to select HFile when not in start and end hour
            has effect on rs, add to master/rs
            home-jira: HBASE-133
        </description>
    </property>

    <property>
        <name>hbase.catalogjanitor.interval</name>
        <value>8640000</value>
        <description>home-jira: HBASE-165</description>
    </property>

    <property>
        <name>hbase.regionserver.replication.handler.count</name>
        <value>0</value>
        <description>home-jira: HBASE-152</description>
    </property>

    <property>
        <name>hbase.regionserver.fileSplitTimeout</name>
        <value>300000</value>
        <description>when split region, will split hfiles, if timeout, will rollback. default value is 30s.
            in guangZhou, region split always timeout, so increase.
            has effect on rs, add to master/rs
            home-jira: HBASE-195
        </description>
    </property>

    <property>
        <name>hbase.regionserver.region.split.threads.max</name>
        <value>100</value>
        <description>when split region, will split hfiles, if timeout, will rollback.
            default not set this value, set too 100 as the thread pool size.
            in guangZhou, region split always timeout, so increase.
            has effect on rs, add to master/rs
            home-jira: HBASE-195
        </description>
    </property>
</configuration>
